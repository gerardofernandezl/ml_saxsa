{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodos del Vecino más cercano\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este bloque se centrará en los métodos del vecino más cercano y cómo implementarlos en TensorFlow. Comenzaremos con una introducción al método y mostraremos cómo implementar varias formas, y el bloque finalizará con ejemplos de coincidencia de direcciones y reconocimiento de imágenes. Esto es lo que cubriremos:\n",
    "\n",
    " 1. Trabajar con vecinos más cercanos\n",
    "\n",
    " 2. Trabajar con distancias basadas en texto\n",
    "\n",
    " 3. Computar Funciones de Distancia Mixta\n",
    "\n",
    " 4. Uso de un ejemplo de coincidencia de direcciones\n",
    "\n",
    " 5. Uso de vecinos más cercanos para el reconocimiento de imágenes\n",
    "\n",
    "<img src=\"images/nearest_neighbor_intro.jpg\" width=\"512\">\n",
    "\n",
    "Los métodos del vecino más cercano se basan en una idea conceptual basada en la distancia. Consideramos nuestro conjunto de entrenamiento como el modelo y hacemos predicciones sobre nuevos puntos en función de lo cerca que están de los puntos del conjunto de entrenamiento. Una forma ingenua es hacer la predicción como la clase de punto de datos de entrenamiento más cercana. Pero como la mayoría de los conjuntos de datos contienen un grado de ruido, un método más común sería tomar un promedio ponderado de un conjunto de k vecinos más cercanos. Este método se llama k-vecinos más cercanos (k-NN).\n",
    "\n",
    "Dado un conjunto de datos de entrenamiento $(x1, x2, ..., xn)$, con los objetivos correspondientes $(y1, y2, ..., yn)$, podemos hacer una predicción sobre un punto, $z$, mirando en un conjunto de vecinos más cercanos. El método real de predicción depende de si estamos haciendo o no regresión (y continua) o clasificación (y discreta).\n",
    "\n",
    "Para objetivos de clasificación discretos, la predicción se puede dar mediante un esquema de votación máximo ponderado por la distancia al punto de predicción:\n",
    "\n",
    "prediction(z) = max ( Suma ponderada de distancias de puntos en cada clase. )\n",
    "\n",
    "Aquí, nuestra predicción es el valor ponderado máximo sobre todas las clases (j), donde la distancia ponderada desde el punto de predicción generalmente viene dada por las funciones de distancia L1 o L2.\n",
    "\n",
    "Los objetivos continuos son muy similares, pero generalmente solo calculamos un promedio ponderado de la variable objetivo (y) por distancia.\n",
    "\n",
    "Hay muchas especificaciones diferentes de métricas de distancia que podemos elegir. En este capítulo, exploraremos las métricas L1 y L2, así como las distancias de edición y de texto.\n",
    "\n",
    "We also have to choose how to weight the distances. A straight forward way to weight the distances is by the distance itself. Points that are further away from our prediction should have less impact than nearer points. The most common way to weight is by the normalized inverse of the distance. We will implement this method in the next recipe.\n",
    "\n",
    "Tenga en cuenta que k-NN es un método de agregación. Para la regresión, estamos realizando un promedio ponderado de vecinos. Debido a esto, las predicciones serán menos extremas y menos variadas que los objetivos reales. La magnitud de este efecto estará determinada por k, el número de vecinos en el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
