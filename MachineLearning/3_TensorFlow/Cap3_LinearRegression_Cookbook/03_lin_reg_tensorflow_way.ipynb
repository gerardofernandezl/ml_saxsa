{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal: El Camino TensorFlow\n",
    "\n",
    "Para este ejercicio, presentamos cómo realizar una regresión lineal en el contexto de TensorFlow.\n",
    "\n",
    "Resolveremos el sistema de ecuaciones lineales:\n",
    "\n",
    "$$ y = Ax + b$$\n",
    "\n",
    "Con la longitud del Sepal (y) y el ancho del Petal (x) de los datos de Iris.\n",
    "\n",
    "Realizar una regresión lineal en TensorFlow es mucho más fácil que tratar de entender las descomposiciones de álgebra o matriz lineal para las dos recetas anteriores. Haremos lo siguiente:\n",
    "\n",
    " 1. Crea la salida del gráfico computacional de regresión lineal. Esto significa que aceptaremos una entrada,  $x$, y generamos la salida, $Ax + b$.\n",
    " 2. Creamos una función de pérdida, la pérdida L2, y usamos esa salida con la tasa de aprendizaje para calcular los gradientes de las variables del modelo, $A$ y $b$ para minimizar la pérdida.\n",
    " \n",
    "La ventaja de utilizar TensorFlow de esta manera es que el modelo se puede actualizar y ajustar de forma rutinaria con nuevos datos de manera incremental con cualquier tamaño de lote razonable. Cuanto más iterativos hacemos nuestros algoritmos de aprendizaje automático, mejor.\n",
    "\n",
    "Comenzamos cargando las bibliotecas necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la sesión del grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, cargamos los datos de Iris de la biblioteca Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\n",
    "iris = datasets.load_iris()\n",
    "x_vals = np.array([x[3] for x in iris.data])\n",
    "y_vals = np.array([y[0] for y in iris.data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la mayoría de los algoritmos de TensorFlow, deberemos declarar un tamaño de lote para los marcadores de posición y las operaciones en el gráfico. Aquí, lo establecemos en 25. Podemos establecerlo en cualquier número entero entre 1 y el tamaño del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar el tamaño del lote (batch)\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora inicializamos los marcadores de posición y las variables en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar los marcadores de posición\n",
    "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Crear las variables para la regresión lineal\n",
    "A = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos las operaciones del modelo (salida del modelo lineal) y la pérdida de L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar las operaciones del modelo\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "\n",
    "# Declarar la función de pérdida (L2 loss)\n",
    "loss = tf.reduce_mean(tf.square(y_target - model_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que decirle a TensorFlow cómo optimizar y volver a propagar los gradientes. Lo hacemos con el operador estándar de descenso de gradiente (`tf.train.GradientDescentOptimizer`), con el argumento de la tasa de aprendizaje de $0.05$.\n",
    "\n",
    "Luego inicializamos todas las variables del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar el optimizador\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.05)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Initializar las variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos nuestro ciclo de entrenamiento y ejecutamos el optimizador para 100 iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ciclo de entrenamiento\n",
    "loss_vec = []\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(len(x_vals), size=batch_size)\n",
    "    rand_x = np.transpose([x_vals[rand_index]])\n",
    "    rand_y = np.transpose([y_vals[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    if (i+1)%25==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n",
    "        print('Loss = ' + str(temp_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los coeficientes óptimos y obtenemos la mejor línea de ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los coeficientes óptimos\n",
    "[slope] = sess.run(A)\n",
    "[y_intercept] = sess.run(b)\n",
    "\n",
    "# Obtener la mejor línea de ajuste\n",
    "best_fit = []\n",
    "for i in x_vals:\n",
    "  best_fit.append(slope*i+y_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafica los resultados con Matplotlib. Junto con el ajuste lineal, también trazaremos la pérdida de L2 sobre las iteraciones de entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar el resultado\n",
    "plt.plot(x_vals, y_vals, 'o', label='Data Points')\n",
    "plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Sepal Length vs Petal Width')\n",
    "plt.xlabel('Petal Width')\n",
    "plt.ylabel('Sepal Length')\n",
    "plt.show()\n",
    "\n",
    "# Graficar la pérdida con el tiempo\n",
    "plt.plot(loss_vec, 'k-')\n",
    "plt.title('L2 Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('L2 Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
