{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal: L1 vs L2\n",
    "-------------------------------\n",
    "\n",
    "Esta función muestra cómo usar TensorFlow para resolver la regresión lineal a través de la matriz inversa.\n",
    "\n",
    "Es importante conocer el efecto de las funciones de pérdida en la convergencia de algoritmos. Aquí ilustraremos cómo las funciones de pérdida de L1 y L2 afectan la convergencia en la regresión lineal. Utilizaremos el mismo conjunto de datos de iris que en la receta anterior, pero cambiaremos nuestras funciones de pérdida y tasas de aprendizaje para ver cómo cambia la convergencia.\n",
    "\n",
    "<img src=\"images/04_L1_L2_learningrates.png\" width=\"512\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos cargando las bibliotecas necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pérdida L1\n",
    "--------------\n",
    "\n",
    "Aquí, ilustraremos la regresión lineal con la pérdida L1. Más adelante en este script, ilustraremos el mismo problema con L2-Loss.\n",
    "\n",
    "La ecuación para la pérdida L1 para cuadrados mínimos lineales es\n",
    "\n",
    "$$S = \\sum_{i=1}^{N}\\left| y_{i} - \\hat{y_{i}} \\right|$$\n",
    "\n",
    "Donde $N$ es el número de puntos de los datos, $y_{i}$ es el  actual i-ésimo valor de y, $\\hat{y_{i}}$ es el valor de y predicho.\n",
    "\n",
    "Comenzamos una sesión del grafo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargamos los datos de Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\n",
    "iris = datasets.load_iris()\n",
    "x_vals = np.array([x[3] for x in iris.data])\n",
    "y_vals = np.array([y[0] for y in iris.data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecer algunos parámetros del modelo.\n",
    "\n",
    "Un parámetro importante a tener en cuenta es la tasa de aprendizaje. Si la velocidad de aprendizaje es demasiado grande, el modelo no convergerá. Si la velocidad de aprendizaje es demasiado pequeña, el modelo convergerá demasiado lentamente.\n",
    "\n",
    "Aquí hay dos valores de tasa de aprendizaje para mostrar la convergencia y la no convergencia.\n",
    "\n",
    "La convergencia ocurre por debajo de 0.35, intente establecer la velocidad de aprendizaje inferior a la de la convergencia. Para ilustrar la no convergencia, establezca la tasa de aprendizaje en 0.4 o superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "learning_rate = 0.4 # No convergerá con la tasa de aprendizaje a 0.4\n",
    "iterations = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos inicializar marcadores de posición, variables de modelo y operaciones de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar los marcadores de posición\n",
    "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Crear las variables para la Regresión lineal\n",
    "A = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Declarar las operaciones del modelo\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, declaramos la función de pérdida de l1 y la función de optimización. Después de eso inicializamos las variables del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar las funciones de pérdida\n",
    "loss_l1 = tf.reduce_mean(tf.abs(y_target - model_output))\n",
    "\n",
    "# Declarar los optimizadores\n",
    "my_opt_l1 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step_l1 = my_opt_l1.minimize(loss_l1)\n",
    "\n",
    "# Inicializar las variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora empezamos el ciclo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ciclo de entrenamiento\n",
    "loss_vec_l1 = []\n",
    "for i in range(iterations):\n",
    "    rand_index = np.random.choice(len(x_vals), size=batch_size)\n",
    "    rand_x = np.transpose([x_vals[rand_index]])\n",
    "    rand_y = np.transpose([y_vals[rand_index]])\n",
    "    sess.run(train_step_l1, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    temp_loss_l1 = sess.run(loss_l1, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec_l1.append(temp_loss_l1)\n",
    "    if (i+1)%25==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pérdida L2\n",
    "--------\n",
    "\n",
    "Aquí, ilustraremos la regresión lineal con la pérdida L2...\n",
    "\n",
    "La ecuación para la pérdida L2 para cuadrados mínimos lineales es\n",
    "\n",
    "$$S = \\sum_{i=1}^{N}\\left( y_{i} - \\hat{y_{i}} \\right)^{2}$$\n",
    "\n",
    "Donde $N$ es el número de puntos de datos, $y_{i}$ es el valor actual del iésimo y, $\\hat{y_{i}}$ es el i-ésimo y predicho.\n",
    "\n",
    "Comenzamos una sesión del grafo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pérdida L2\n",
    "# Reiniciar el grafo\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Crear el grafo\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que antes, inicializamos los marcadores de posición, las variables y las operaciones de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar los marcacdores de posición\n",
    "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Crear las variables para la Regresión lineal\n",
    "A = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Declarar las operaciones del modelo\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí están las funciones de pérdida, inicialización de variables y funciones de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar las funciones de pérdida\n",
    "loss_l2 = tf.reduce_mean(tf.square(y_target - model_output))\n",
    "\n",
    "#Declarar los optimizadores\n",
    "my_opt_l2 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step_l2 = my_opt_l2.minimize(loss_l2)\n",
    "\n",
    "# Inicializar las variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos comenzar nuestro entrenamiento de regresión lineal con la función L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vec_l2 = []\n",
    "for i in range(iterations):\n",
    "    rand_index = np.random.choice(len(x_vals), size=batch_size)\n",
    "    rand_x = np.transpose([x_vals[rand_index]])\n",
    "    rand_y = np.transpose([y_vals[rand_index]])\n",
    "    sess.run(train_step_l2, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    temp_loss_l2 = sess.run(loss_l2, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec_l2.append(temp_loss_l2)\n",
    "    if (i+1)%25==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí está el código matplotlib para trazar la pérdida de las funciones de pérdida L1 y L2 aplicadas al mismo problema de regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la pérdida a través del tiempo\n",
    "plt.plot(loss_vec_l1, 'k-', label='L1 Loss')\n",
    "plt.plot(loss_vec_l2, 'r--', label='L2 Loss')\n",
    "plt.title('L1 and L2 Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('L1 Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
