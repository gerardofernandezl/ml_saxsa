{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Crear carpeta Mllib en hadoop\n",
    "\n",
    "hdfs dfs -mkdir /FuentesMllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Subir archivos spam.txt y normal.txt a la carpeta FuentesMllib en hadoop\n",
    "\n",
    "hdfs dfs -copyFromLocal spam.txt /FuentesMllib\n",
    "hdfs dfs -copyFromLocal normal.txt /FuentesMllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import SVMWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer archivos \n",
    "\n",
    "spam = sc.textFile(\"/FuentesMllib/spam.txt\")\n",
    "normal = sc.textFile(\"/FuentesMllib/normal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una instancia HashingTF para asignar un numero hasta 10000 palabras diferentes \n",
    "\n",
    "tf = HashingTF(numFeatures = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se dividen los archivos por palabras y a cada palabra  se le aplica la transformacion a numero\n",
    "\n",
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "normalFeatures = normal.map(lambda email: tf.transform(email.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso se crean conjuntos de datos con una estructura LabeledPoint \n",
    "# Se asigna 1 a spam y 0 a normal\n",
    "\n",
    "positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "negativeExamples = normalFeatures.map(lambda features: LabeledPoint(0, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las etiquetas son:\n",
    "\n",
    "## SPAM = 1\n",
    "## NORMAL = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se hace la Union de los LabeledPoint\n",
    "\n",
    "trainingData = positiveExamples.union(negativeExamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecuta algoritmo SVM (Support Vector Machine) con SGD () para hacer entrenamiento\n",
    "\n",
    "model = SVMWithSGD.train(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probamos para un ejemplo positivo de spam y luego uno normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos la misma transformaci√≥n HashingTF para obtener los vectores y luego aplicamos el modelo\n",
    "\n",
    "posTest = tf.transform(\"Felicidades has ganado solo da clic ...\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos el resultado para positivo en SPAM\n",
    "\n",
    "print (\"Prediction for positive test example: %g\" % model.predict(posTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos el resultado correo NORMAL\n",
    "negTest = tf.transform(\"Hola Israel ya empece con Spark Machine Learning ...\".split(\" \"))\n",
    "\n",
    "print (\"Prediction for negative test example: %g\" % model.predict(negTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
